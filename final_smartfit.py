# -*- coding: utf-8 -*-
"""Final Smartfit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wb6l6uVqUHFHYAg4W8ND9wXlAuw-ESqV

#Libraries
"""

!pip install python_resize_image
!pip install ninja
!git clone https://github.com/PeikeLi/Self-Correction-Human-Parsing
#Test
!pip install tensorboardX

!git clone https://github.com/SMART-FIT/smartfit.git

#flask
!pip install flask-ngrok
!pip install flask_uploads
!pip install Werkzeug==0.16.1

"""#Final All Files"""

from flask_ngrok import run_with_ngrok
#from flask import Flask
#from flask import Flask,render_template
from flask import Flask, request, render_template, url_for ,redirect ,send_from_directory
from flask_uploads import UploadSet, configure_uploads, IMAGES
from werkzeug.utils import secure_filename

import os
app = Flask(__name__,template_folder='/content/drive/MyDrive/FlaskFinal/Smartfit/templates', static_folder='/content/drive/MyDrive/FlaskFinal/Smartfit/static')

#app.run(host='0.0.0.1',port=5000)
run_with_ngrok(app)   #starts ngrok when the app is run


photos = UploadSet('photos', IMAGES)

app.config['UPLOADED_PHOTOS_DEST'] = '/content/drive/MyDrive/finalSmartfit/test/cloth'
app.config['UPLOADED_PHOTOS_DEST1'] = '/content/drive/MyDrive/finalSmartfit/test/image'
app.config['UPLOADED_PHOTOS_DEST3'] = '/content/drive/MyDrive/finalSmartfit/test/try2-on'

configure_uploads(app, photos)
@app.route("/home")
@app.route("/")
def home():
	return render_template("home.html")

@app.route("/login")
def login():
	return render_template("login.html")
"""
@app.route("/output/<filename>")
def output(filename):
	return send_from_directory(app.config['UPLOADED_PHOTOS_DEST'],
							   filename)
"""
@app.route("/register")
def register():
	return render_template("register.html")





# from flask import send_from_directory

@app.route('/uploads/<filename>')
def uploaded_file(filename):
	return send_from_directory(app.config['UPLOADED_PHOTOS_DEST3'],
							   filename)


@app.route('/upload', methods=['GET', 'POST'])
def upload():
  if request.method == 'POST' and 'photo' in request.files:
    file= request.files['photo']
    filename = secure_filename(file.filename)
    filename="000010_1.jpg"
    file.save(os.path.join(app.config['UPLOADED_PHOTOS_DEST'], filename))
    #filename = photos.save(request.files['photo'])
    #filename2 = photos.save(request.files['model'])
    file1 = request.files['model']
    filename2 = secure_filename(file1.filename)
    filename2="000001_0.jpg"
    file1.save(os.path.join(app.config['UPLOADED_PHOTOS_DEST1'], filename2))
    Smartfit()
    return redirect(url_for('uploaded_file', filename=filename2))
  return render_template('upload.html')


def Smartfit():
  import numpy as np
  import cv2
  from matplotlib import pyplot as plt
  from PIL import Image
  from resizeimage import resizeimage
  import cv2
  import time
  import numpy as np
  from random import randint
  import argparse
  import json 

  image = Image.open("/content/drive/MyDrive/finalSmartfit/test/cloth/000010_1.jpg")
  resized_image = image.resize((192,256))
  resized_image.save("/content/drive/MyDrive/finalSmartfit/test/cloth/000010_1.jpg")
  image = Image.open("/content/drive/MyDrive/finalSmartfit/test/image/000001_0.jpg")
  resized_image = image.resize((192,256))
  resized_image.save("/content/drive/MyDrive/finalSmartfit/test/image/000001_0.jpg")
  def Cloth_Mask(input_image_location,output_image_location):
      PATH_IMG = input_image_location
      image = cv2.imread(PATH_IMG)
      #cv2.threshold(image, 250, 255, cv2.THRESH_BINARY_INV)
      mask = np.zeros(image.shape[:2], dtype="uint8")
      rect = (1, 1, mask.shape[1], mask.shape[0])
      fgModel = np.zeros((1, 65), dtype="float")
      bgModel = np.zeros((1, 65), dtype="float")
      #start = time.time()
      (mask, bgModel, fgModel) = cv2.grabCut(image, mask, rect, bgModel,
                                            fgModel, iterCount=10, mode=cv2.GC_INIT_WITH_RECT)
      outputMask = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD),0, 1)
      outputMask = (outputMask).astype("uint8")
      plt.axis('off')
      plt.imshow(outputMask, cmap="gray")
      plt.savefig(output_image_location, bbox_inches='tight', pad_inches = 0)
      image = Image.open(output_image_location)
      resized_image = image.resize((192,256))
      resized_image.save(output_image_location)
      image = Image.open(output_image_location)

  def Model_Mask(input_image_location,output_image_location):
      PATH_IMG = input_image_location
      image = cv2.imread(PATH_IMG)
      #cv2.threshold(image, 250, 255, cv2.THRESH_BINARY_INV)
      mask = np.zeros(image.shape[:2], dtype="uint8")
      rect = (1, 1, mask.shape[1], mask.shape[0])
      fgModel = np.zeros((1, 65), dtype="float")
      bgModel = np.zeros((1, 65), dtype="float")
      #start = time.time()
      (mask, bgModel, fgModel) = cv2.grabCut(image, mask, rect, bgModel,
                                            fgModel, iterCount=10, mode=cv2.GC_INIT_WITH_RECT)
      outputMask = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD),0, 1)
      outputMask = (outputMask).astype("uint8")
      plt.axis('off')
      plt.imshow(outputMask, cmap="gray")
      plt.savefig(output_image_location, bbox_inches='tight', pad_inches = 0)
      image = Image.open(output_image_location)
      resized_image = image.resize((192,256))
      resized_image.save(output_image_location)
      image = Image.open(output_image_location)





  def generate_keypoints_json(input_image_location,protoFile_location,weightsFile_loaction,output_json_location):
      image1 = cv2.imread(input_image_location)
      protoFile = protoFile_location
      weightsFile = weightsFile_loaction
      nPoints = 18
      # COCO Output Format
      keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', 'R-Wr', 'L-Sho', 'L-Elb', 'L-Wr', 'R-Hip', 'R-Knee', 'R-Ank', 'L-Hip', 'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', 'R-Ear', 'L-Ear']

      POSE_PAIRS = [[1,2], [1,5], [2,3], [3,4], [5,6], [6,7],
                    [1,8], [8,9], [9,10], [1,11], [11,12], [12,13],
                    [1,0], [0,14], [14,16], [0,15], [15,17],
                    [2,17], [5,16] ]

      # index of pafs correspoding to the POSE_PAIRS
      # e.g for POSE_PAIR(1,2), the PAFs are located at indices (31,32) of output, Similarly, (1,5) -> (39,40) and so on.
      mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44],
                [19,20], [21,22], [23,24], [25,26], [27,28], [29,30],
                [47,48], [49,50], [53,54], [51,52], [55,56],
                [37,38], [45,46]]

      colors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],
              [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],
              [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]

      def getKeypoints(probMap, threshold=0.1):

          mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)

          mapMask = np.uint8(mapSmooth>threshold)
          keypoints = []

          #find the blobs
          contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

          #for each blob find the maxima
          for cnt in contours:
              blobMask = np.zeros(mapMask.shape)
              blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)
              maskedProbMap = mapSmooth * blobMask
              _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)
              keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],))

          return keypoints


      # Find valid connections between the different joints of a all persons present
      def getValidPairs(output):
          valid_pairs = []
          invalid_pairs = []
          n_interp_samples = 10
          paf_score_th = 0.1
          conf_th = 0.7
          # loop for every POSE_PAIR
          for k in range(len(mapIdx)):
              # A->B constitute a limb
              pafA = output[0, mapIdx[k][0], :, :]
              pafB = output[0, mapIdx[k][1], :, :]
              pafA = cv2.resize(pafA, (frameWidth, frameHeight))
              pafB = cv2.resize(pafB, (frameWidth, frameHeight))

              # Find the keypoints for the first and second limb
              candA = detected_keypoints[POSE_PAIRS[k][0]]
              candB = detected_keypoints[POSE_PAIRS[k][1]]
              nA = len(candA)
              nB = len(candB)

              # If keypoints for the joint-pair is detected
              # check every joint in candA with every joint in candB
              # Calculate the distance vector between the two joints
              # Find the PAF values at a set of interpolated points between the joints
              # Use the above formula to compute a score to mark the connection valid

              if( nA != 0 and nB != 0):
                  valid_pair = np.zeros((0,3))
                  for i in range(nA):
                      max_j=-1
                      maxScore = -1
                      found = 0
                      for j in range(nB):
                          # Find d_ij
                          d_ij = np.subtract(candB[j][:2], candA[i][:2])
                          norm = np.linalg.norm(d_ij)
                          if norm:
                              d_ij = d_ij / norm
                          else:
                              continue
                          # Find p(u)
                          interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples),
                                                  np.linspace(candA[i][1], candB[j][1], num=n_interp_samples)))
                          # Find L(p(u))
                          paf_interp = []
                          for k in range(len(interp_coord)):
                              paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))],
                                                pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))] ])
                          # Find E
                          paf_scores = np.dot(paf_interp, d_ij)
                          avg_paf_score = sum(paf_scores)/len(paf_scores)

                          # Check if the connection is valid
                          # If the fraction of interpolated vectors aligned with PAF is higher then threshold -> Valid Pair
                          if ( len(np.where(paf_scores > paf_score_th)[0]) / n_interp_samples ) > conf_th :
                              if avg_paf_score > maxScore:
                                  max_j = j
                                  maxScore = avg_paf_score
                                  found = 1
                      # Append the connection to the list
                      if found:
                          valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], maxScore]], axis=0)

                  # Append the detected connections to the global list
                  valid_pairs.append(valid_pair)
              else: # If no keypoints are detected
                  print("No Connection : k = {}".format(k))
                  invalid_pairs.append(k)
                  valid_pairs.append([])
          return valid_pairs, invalid_pairs


      # This function creates a list of keypoints belonging to each person
      # For each detected valid pair, it assigns the joint(s) to a person
      def getPersonwiseKeypoints(valid_pairs, invalid_pairs):
          # the last number in each row is the overall score
          personwiseKeypoints = -1 * np.ones((0, 19))

          for k in range(len(mapIdx)):
              if k not in invalid_pairs:
                  partAs = valid_pairs[k][:,0]
                  partBs = valid_pairs[k][:,1]
                  indexA, indexB = np.array(POSE_PAIRS[k])

                  for i in range(len(valid_pairs[k])):
                      found = 0
                      person_idx = -1
                      for j in range(len(personwiseKeypoints)):
                          if personwiseKeypoints[j][indexA] == partAs[i]:
                              person_idx = j
                              found = 1
                              break

                      if found:
                          personwiseKeypoints[person_idx][indexB] = partBs[i]
                          personwiseKeypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + valid_pairs[k][i][2]

                      # if find no partA in the subset, create a new subset
                      elif not found and k < 17:
                          row = -1 * np.ones(19)
                          row[indexA] = partAs[i]
                          row[indexB] = partBs[i]
                          # add the keypoint_scores for the two keypoints and the paf_score
                          row[-1] = sum(keypoints_list[valid_pairs[k][i,:2].astype(int), 2]) + valid_pairs[k][i][2]
                          personwiseKeypoints = np.vstack([personwiseKeypoints, row])
          return personwiseKeypoints


      frameWidth = image1.shape[1]
      frameHeight = image1.shape[0]

      t = time.time()
      net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)
      #if args.device == "cpu":
      net.setPreferableBackend(cv2.dnn.DNN_TARGET_CPU)
      print("Using CPU device")
      # elif args.device == "gpu":
      #     net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
      #     net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)
      #     print("Using GPU device")

      # Fix the input Height and get the width according to the Aspect Ratio
      inHeight = 368
      inWidth = int((inHeight/frameHeight)*frameWidth)

      inpBlob = cv2.dnn.blobFromImage(image1, 1.0 / 255, (inWidth, inHeight),
                                (0, 0, 0), swapRB=False, crop=False)

      net.setInput(inpBlob)
      output = net.forward()
      print("Time Taken in forward pass = {}".format(time.time() - t))

      detected_keypoints = []
      keypoints_list = np.zeros((0,3))
      keypoint_id = 0
      threshold = 0.1

      for part in range(nPoints):
          probMap = output[0,part,:,:]
          probMap = cv2.resize(probMap, (image1.shape[1], image1.shape[0]))
          keypoints = getKeypoints(probMap, threshold)
          print("Keypoints - {} : {}".format(keypointsMapping[part], keypoints))
          keypoints_with_id = []
          for i in range(len(keypoints)):
              keypoints_with_id.append(keypoints[i] + (keypoint_id,))
              keypoints_list = np.vstack([keypoints_list, keypoints[i]])
              keypoint_id += 1

          detected_keypoints.append(keypoints_with_id)


      frameClone = image1.copy()
      pose_keypoints = []
      for i in range(nPoints):
          if detected_keypoints[i] ==[]:
              pose_keypoints.append(0)
              pose_keypoints.append(0)
              pose_keypoints.append(0)       

          for j in range(len(detected_keypoints[i])):

              pose_keypoints.append(detected_keypoints[i][j][0])
              pose_keypoints.append(detected_keypoints[i][j][1])
              pose_keypoints.append(detected_keypoints[i][j][2].astype(float))
              cv2.circle(frameClone, detected_keypoints[i][j][0:2], 5, colors[i], -1, cv2.LINE_AA)

      json_data = {"version": 1.0, "people": [
                  {"face_keypoints": [],
                  "pose_keypoints":pose_keypoints, 
                  "hand_right_keypoints": [], 
                  "hand_left_keypoints": []
                  }]}

      with open(output_json_location, 'w') as outfile:
          json.dump(json_data, outfile)


  #!/usr/bin/env python
  # -*- encoding: utf-8 -*-

  """
  @Author  :   Peike Li
  @Contact :   peike.li@yahoo.com
  @File    :   simple_extractor.py
  @Time    :   8/30/19 8:59 PM
  @Desc    :   Simple Extractor
  @License :   This source code is licensed under the license found in the
              LICENSE file in the root directory of this source tree.
  """

  import os
  import torch
  import argparse
  import numpy as np
  from PIL import Image
  from tqdm import tqdm

  from torch.utils.data import DataLoader
  import torchvision.transforms as transforms
  import sys  
  sys.path.append("/content/Self-Correction-Human-Parsing/");
  import networks
  from utils.transforms import transform_logits
  from datasets.simple_extractor_dataset import SimpleFolderDataset

  dataset_settings = {
      'lip': {
          'input_size': [473, 473],
          'num_classes': 20,
          'label': ['Background', 'Hat', 'Hair', 'Glove', 'Sunglasses', 'Upper-clothes', 'Dress', 'Coat',
                    'Socks', 'Pants', 'Jumpsuits', 'Scarf', 'Skirt', 'Face', 'Left-arm', 'Right-arm',
                    'Left-leg', 'Right-leg', 'Left-shoe', 'Right-shoe']
      },
      'atr': {
          'input_size': [512, 512],
          'num_classes': 18,
          'label': ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt',
                    'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']
      },
      'pascal': {
          'input_size': [512, 512],
          'num_classes': 7,
          'label': ['Background', 'Head', 'Torso', 'Upper Arms', 'Lower Arms', 'Upper Legs', 'Lower Legs'],
      }
  }

  dataset = 'lip'         #select from ['lip', 'atr', 'pascal']
  def get_arguments(model_path_lip,input_directory,output_directory):
      """Parse all the arguments provided from the CLI.
      Returns:
        A list of parsed arguments.
      """
      parser = argparse.ArgumentParser(description="Self Correction for Human Parsing")
      parser.add_argument('-f')
      parser.add_argument("--dataset", type=str, default='lip')
      parser.add_argument("--model-restore", type=str, default=model_path_lip, help="restore pretrained model parameters.")

      parser.add_argument("--gpu", type=str, default='0', help="choose gpu device.")
      parser.add_argument("--input-dir", type=str, default=input_directory, help="path of input image folder.")
      parser.add_argument("--output-dir", type=str, default=output_directory, help="path of output image folder.")

      parser.add_argument("--logits", action='store_true', default=False, help="whether to save the logits.")
      return parser.parse_args()

  def get_palette(num_cls):
      """ Returns the color map for visualizing the segmentation mask.
      Args:
          num_cls: Number of classes
      Returns:
          The color map
      """
      n = num_cls
      palette = [0] * (n * 3)
      for j in range(0, n):
          lab = j
          palette[j * 3 + 0] = 0
          palette[j * 3 + 1] = 0
          palette[j * 3 + 2] = 0
          i = 0
          while lab:
              palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))
              palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))
              palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))
              i += 1
              lab >>= 3
      return palette


  def human_parsing(human_lip,input_directory,output_directory):
      args = get_arguments(human_lip,input_directory,output_directory)

      #print(args)
      gpus = [int(i) for i in args.gpu.split(',')]
      assert len(gpus) == 1
      if not args.gpu == 'None':
          os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu

      num_classes = dataset_settings[args.dataset]['num_classes']
      input_size = dataset_settings[args.dataset]['input_size']
      label = dataset_settings[args.dataset]['label']
      print("Evaluating total class number {} with {}".format(num_classes, label))

      model = networks.init_model('resnet101', num_classes=num_classes, pretrained=None)

      state_dict = torch.load(args.model_restore)['state_dict']
      from collections import OrderedDict
      new_state_dict = OrderedDict()
      for k, v in state_dict.items():
          name = k[7:]  # remove `module.`
          new_state_dict[name] = v
      model.load_state_dict(new_state_dict)
      model.cuda()
      model.eval()

      transform = transforms.Compose([
          transforms.ToTensor(),
          transforms.Normalize(mean=[0.406, 0.456, 0.485], std=[0.225, 0.224, 0.229])
      ])
      dataset = SimpleFolderDataset(root=args.input_dir, input_size=input_size, transform=transform)
      dataloader = DataLoader(dataset)

      if not os.path.exists(args.output_dir):
          os.makedirs(args.output_dir)

      palette = get_palette(num_classes)
      with torch.no_grad():
          for idx, batch in enumerate(tqdm(dataloader)):
              image, meta = batch
              img_name = meta['name'][0]
              c = meta['center'].numpy()[0]
              s = meta['scale'].numpy()[0]
              w = meta['width'].numpy()[0]
              h = meta['height'].numpy()[0]

              output = model(image.cuda())
              upsample = torch.nn.Upsample(size=input_size, mode='bilinear', align_corners=True)
              upsample_output = upsample(output[0][-1][0].unsqueeze(0))
              upsample_output = upsample_output.squeeze()
              upsample_output = upsample_output.permute(1, 2, 0)  # CHW -> HWC

              logits_result = transform_logits(upsample_output.data.cpu().numpy(), c, s, w, h, input_size=input_size)
              parsing_result = np.argmax(logits_result, axis=2)
              parsing_result_path = os.path.join(args.output_dir, img_name[:-4] + '.png')
              output_img = Image.fromarray(np.asarray(parsing_result, dtype=np.uint8))
              output_img.putpalette(palette)
              output_img.save(parsing_result_path)
              #if args.logits:
                  #logits_result_path = os.path.join(args.output_dir, img_name[:-4] + '.npy')
                  #np.save(logits_result_path, logits_result)
      return

  """
  Make updated body segmentation with new neck/skin label
  """


  import os
  import numpy as np
  import cv2
  from PIL import Image
  from matplotlib import pyplot as plt
  import sys
  import shutil

  N_CLASSES = 21
  fine_width = 192
  fine_height = 256

  # colour map for LIP dataset (plus extra)
  label_colours = [(0, 0, 0),  # 0=Background
                  (128, 0, 0),  # 1=Hat
                  (255, 0, 0),  # 2=Hair
                  (0, 85, 0),   # 3=Glove
                  (170, 0, 51),  # 4=Sunglasses
                  (255, 85, 0),  # 5=UpperClothes
                  (0, 0, 85),  # 6=Dress
                  (0, 119, 221),  # 7=Coat
                  (85, 85, 0),  # 8=Socks
                  (0, 85, 85),  # 9=Pants
                  (85, 51, 0),  # 10=Jumpsuits
                  (52, 86, 128),  # 11=Scarf
                  (0, 128, 0),  # 12=Skirt
                  (0, 0, 255),  # 13=Face
                  (51, 170, 221),  # 14=LeftArm
                  (0, 255, 255),  # 15=RightArm
                  (85, 255, 170),  # 16=LeftLeg
                  (170, 255, 85),  # 17=RightLeg
                  (255, 255, 0),  # 18=LeftShoe
                  (255, 170, 0),  # 19=RightShoe
                  (189, 183, 107)  # 20=Neck    # new added
                  ]

  (cv_major, _, _) = cv2.__version__.split(".")
  if cv_major != '4' and cv_major != '3':
      print('doesnot support opencv version')
      sys.exit()


  def decode_labels(mask):


      mask = np.expand_dims(mask, axis=2)
      h, w, c = mask.shape

      outputs = np.zeros((h, w, 3), dtype=np.uint8)

      par_img = Image.new('RGB', (w, h))
      pixels = par_img.load()
      for j_, j in enumerate(mask[:, :, 0]):
          for k_, k in enumerate(j):
              if k < N_CLASSES:
                  pixels[k_, j_] = label_colours[k]
      outputs = np.array(par_img)

      return outputs



  def body_detection(image, seg_mask):
      # binary thresholding by blue ?
      hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
      lower_blue = np.array([0, 0, 120])
      upper_blue = np.array([180, 38, 255])
      mask = cv2.inRange(hsv, lower_blue, upper_blue)
      result = cv2.bitwise_and(image, image, mask=mask)

      # binary threshold by green ?
      b, g, r = cv2.split(result)
      filter = g.copy()
      ret, mask = cv2.threshold(filter, 10, 255, 1)

      # at least original segmentation is FG
      mask[seg_mask] = 1

      return mask


  def shape_from_contour(img, contour):
      dummy_mask = np.zeros((img.shape[0], img.shape[1], 3))
      dummy_mask = cv2.drawContours(
          dummy_mask, [contour], 0, (1, 0, 0), thickness=cv2.FILLED)
      x, y = np.where(dummy_mask[:, :, 0] == 1)
      inside_points = np.stack((x, y), axis=-1)
      return inside_points


  def update_image_segmentation(data_dir, mask_dir, image_name, mask_name, save_dir=None, save_vis=True):
      print(image_name)

      # define paths
      img_pth = os.path.join(data_dir, image_name)
      seg_pth = os.path.join(mask_dir, mask_name)

      updated_seg_pth = None
      updated_seg_vis_pth = None
      if save_dir is not None:
          updated_seg_pth = os.path.join(save_dir, mask_name)
          if save_vis:
              updated_seg_vis_pth = updated_seg_pth.replace("image-parse-new", "image-parse-new-vis")
              if not os.path.exists(updated_seg_vis_pth):
                  os.makedirs(updated_seg_vis_pth)

      # Load image and make binary body mask
      img = cv2.imread(img_pth)

      # Load the segmentation in grayscale and make binary mask
      segmentation = Image.open(seg_pth)

      # the png file should be 1-ch but it is 3 ch ^^;
      gray = cv2.imread(seg_pth, cv2.IMREAD_GRAYSCALE)
      # print('shape of seg:', seg_pth, ':', gray.shape)
      # _, seg_mask = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)  # why 10? bg is 0
      _, seg_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)

      body_mask = body_detection(img, seg_mask)

      # Get the neck/skin region (plus extra mis-segmented)
      upper_body = body_mask - seg_mask
      upper_body[upper_body > 0] = 20
      upper_body_vis = upper_body.copy()

      
      height, width = upper_body.shape
      upper_body[height//2:, :] = 0
      # noise reduction

      # get contours
      if cv_major == '4':
          contours, hier = cv2.findContours(
              upper_body, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
      elif cv_major == '3':
          _, contours, hier = cv2.findContours(
              upper_body, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
      else:
          return

      neck = None

      if len(contours) > 0:
          # draw in blue the contours that were founded
          cv2.drawContours(upper_body_vis, contours, -1, 255, 3)

          # find the biggest area
          c_neck = max(contours, key=cv2.contourArea)

          neck = shape_from_contour(img, c_neck)

          x, y, w, h = cv2.boundingRect(c_neck)
          # draw the book contour (in green)
          cv2.rectangle(upper_body_vis, (x, y), (x + w, y + h), (170, 230, 0), 2)

      # make neck region mask
      neck_mask = np.zeros((fine_height, fine_width)).astype(np.int)
      for each in neck:
          neck_mask[each[0]][each[1]] = 20

      # Add neck/skin to segmentation
      result = segmentation + neck_mask

      # handle overlapped pixels
      for i in range(1, 20):
          result[result == 20 + i] = i

      # save new segmentation
      if updated_seg_pth is not None:
          cv2.imwrite(updated_seg_pth, result)
          if save_vis:
              msk = decode_labels(result)
              parsing_im = Image.fromarray(msk)
              parsing_im.save('{}/{}_vis.png'.format(updated_seg_vis_pth, mask_name[:-4]))
      else:  # display for checking

          plt.suptitle(image_name)
          plt.subplot(1, 4, 1)
          plt.title("input")
          plt.axis('off')
          plt.imshow(img[:, :, ::-1])
          plt.subplot(1, 4, 2)
          plt.title("body silhouette")
          plt.axis('off')
          plt.imshow(body_mask)
          plt.subplot(1, 4, 3)
          plt.title("orig. mask")
          plt.axis('off')
          plt.imshow(segmentation)
          plt.subplot(1, 4, 4)
          plt.title("relabeled")
          plt.axis('off')
          msk = decode_labels(result)         # ???
          parsing_im = Image.fromarray(msk)   # ???
          plt.imshow(parsing_im)
          plt.show()


  def Body_Segmentation():
      # define paths

      root_dir = "/content/drive/MyDrive/finalSmartfit"
      updated_seg_folder = "image-parse-new"

      # data_mode = "train"
      data_mode = "test"
      image_folder = "image"
      seg_folder = "image-parse"

      image_dir = os.path.join(os.path.join(root_dir, data_mode), image_folder)
      seg_dir = os.path.join(os.path.join(root_dir, data_mode), seg_folder)
      if updated_seg_folder is not None:
          updated_seg_dir = os.path.join(os.path.join(
              root_dir, data_mode), updated_seg_folder)
          if not os.path.exists(updated_seg_dir):
              os.makedirs(updated_seg_dir)
      else:
          updated_seg_dir = None

      image_list = sorted(os.listdir(image_dir))
      masks_list = sorted(os.listdir(seg_dir))

      try:
          shutil.rmtree(os.path.join(image_dir, '.ipynb_checkpoints'))
          shutil.rmtree(os.path.join(seg_dir, '.ipynb_checkpoints'))
      except:
          print("Clean")

      for each in zip(image_list, masks_list):
          mask = each[0].replace("jpg", "png")
          update_image_segmentation(
              image_dir, seg_dir, each[0], mask, updated_seg_dir)


  """
  Make updated body shape from updated segmentation
  """

  import os
  import numpy as np
  import cv2
  from PIL import Image
  import sys


  (cv_major, _, _) = cv2.__version__.split(".")
  if cv_major != '4' and cv_major != '3':
      print('doesnot support opencv version')
      sys.exit()


  # @TODO this is too simple and pixel based algorithm
  def body_detection(image, seg_mask):
      # binary thresholding by blue ?
      hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
      lower_blue = np.array([0, 0, 120])
      upper_blue = np.array([180, 38, 255])
      mask = cv2.inRange(hsv, lower_blue, upper_blue)
      result = cv2.bitwise_and(image, image, mask=mask)

      # binary threshold by green ?
      b, g, r = cv2.split(result)
      filter = g.copy()
      ret, mask = cv2.threshold(filter, 10, 255, 1)

      # at least original segmentation is FG
      mask[seg_mask] = 1

      return mask


  def make_body_mask(data_dir, seg_dir, image_name, mask_name, save_dir=None):
      print(image_name)

      # define paths
      img_pth = os.path.join(data_dir, image_name)
      seg_pth = os.path.join(seg_dir, mask_name)

      mask_path = None
      if save_dir is not None:
          mask_path = os.path.join(save_dir, mask_name)

      # Load images
      img = cv2.imread(img_pth)
      # segm = Image.open(seg_pth)
      # the png file should be 1-ch but it is 3 ch ^^;
      gray = cv2.imread(seg_pth, cv2.IMREAD_GRAYSCALE)
      _, seg_mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)

      body_mask = body_detection(img, seg_mask)
      body_mask = body_mask + seg_mask
      body_mask[seg_mask] = 1
      cv2.imwrite(mask_path, body_mask)


  def Body_Masking():
      # define paths

      # root_dir = "data/viton_resize"
      root_dir = "/content/drive/MyDrive/finalSmartfit/"
      mask_folder = "image-mask"
      seg_folder = "image-parse-new"

      # data_mode = "train"
      data_mode = "test"
      image_folder = "image"

      image_dir = os.path.join(os.path.join(root_dir, data_mode), image_folder)
      seg_dir = os.path.join(os.path.join(root_dir, data_mode), seg_folder)

      image_list = sorted(os.listdir(image_dir))
      seg_list = sorted(os.listdir(seg_dir))

      mask_dir = os.path.join(os.path.join(root_dir, data_mode), mask_folder)
      if not os.path.exists(mask_dir):
          os.makedirs(mask_dir)

      for each in zip(image_list, seg_list):
          make_body_mask(image_dir, seg_dir, each[0], each[1], mask_dir)


  #Testing
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  import argparse
  import os
  import time
  import sys
  sys.executable
  sys.argv=['']
  sys.path.insert(0,'/content/drive/MyDrive/smartfit')
  sys.path.append('/content/drive/MyDrive/smartfit')

  from cp_dataset import CPDataset, CPDataLoader
  from networks2 import GMM, UnetGenerator, load_checkpoint

  from tensorboardX import SummaryWriter
  from visualization import board_add_image, board_add_images, save_images


  def get_gopt():
      parser = argparse.ArgumentParser()
      parser.add_argument('-f')
      parser.add_argument("--name", default="GMM")

      parser.add_argument("--gpu_ids", default="")
      parser.add_argument('-j', '--workers', type=int, default=1)
      parser.add_argument('-b', '--batch-size', type=int, default=4)

      parser.add_argument("--dataroot", default="/content/drive/MyDrive/finalSmartfit/")

      parser.add_argument("--datamode", default="test")

      parser.add_argument("--stage", default="GMM")

      parser.add_argument("--data_list", default="/content/drive/MyDrive/finalSmartfit/test_pairs.txt")

      parser.add_argument("--fine_width", type=int, default=192)
      parser.add_argument("--fine_height", type=int, default=256)
      parser.add_argument("--radius", type=int, default=5)
      parser.add_argument("--grid_size", type=int, default=5)

      parser.add_argument('--tensorboard_dir', type=str,
                          default='tensorboard', help='save tensorboard infos')

      parser.add_argument('--result_dir', type=str,
                          default='/content/drive/MyDrive/finalSmartfit/test', help='save result infos')

      parser.add_argument('--checkpoint', type=str, default='/content/drive/MyDrive/gmm_final.pth', help='model checkpoint for test')

      parser.add_argument("--display_count", type=int, default=1)
      parser.add_argument("--shuffle", action='store_true',
                          help='shuffle input data')

      gopt = parser.parse_args()
      return gopt

  def get_topt():
      parser = argparse.ArgumentParser()
      parser.add_argument('-f')
      parser.add_argument("--name", default="TOM")

      parser.add_argument("--gpu_ids", default="")
      parser.add_argument('-j', '--workers', type=int, default=1)
      parser.add_argument('-b', '--batch-size', type=int, default=4)

      parser.add_argument("--dataroot", default="/content/drive/MyDrive/finalSmartfit/")
      parser.add_argument("--datamode", default="test")

      parser.add_argument("--stage", default="TOM")
      parser.add_argument("--data_list", default="/content/drive/MyDrive/finalSmartfit/test_pairs.txt")

      parser.add_argument("--fine_width", type=int, default=192)
      parser.add_argument("--fine_height", type=int, default=256)
      parser.add_argument("--radius", type=int, default=5)
      parser.add_argument("--grid_size", type=int, default=5)

      parser.add_argument('--tensorboard_dir', type=str,
                          default='tensorboard', help='save tensorboard infos')

      parser.add_argument('--result_dir', type=str,
                          default='/content/drive/MyDrive/finalSmartfit/test', help='save result infos')

      parser.add_argument('--checkpoint', type=str, default='/content/drive/MyDrive/tom_final.pth', help='model checkpoint for test')

      parser.add_argument("--display_count", type=int, default=1)
      parser.add_argument("--shuffle", action='store_true',
                          help='shuffle input data')

      topt = parser.parse_args()
      return topt


  def test_gmm(opt, test_loader, model, board):
      # model.cuda()
      # model.eval()

      base_name = os.path.basename(opt.checkpoint)
      name = opt.name
      save_dir = '/content/drive/MyDrive/finalSmartfit/test/'
      print(save_dir)
      #save_dir = os.path.join(opt.result_dir, name, opt.datamode)
      if not os.path.exists(save_dir):
          os.makedirs(save_dir)
      warp_cloth_dir = os.path.join(save_dir, 'warp-cloth')
      if not os.path.exists(warp_cloth_dir):
          os.makedirs(warp_cloth_dir)
      warp_mask_dir = os.path.join(save_dir, 'warp-mask')
      if not os.path.exists(warp_mask_dir):
          os.makedirs(warp_mask_dir)
      result_dir1 = os.path.join(save_dir, 'result_dir')
      if not os.path.exists(result_dir1):
          os.makedirs(result_dir1)
      overlayed_TPS_dir = os.path.join(save_dir, 'overlayed_TPS')
      if not os.path.exists(overlayed_TPS_dir):
          os.makedirs(overlayed_TPS_dir)
      warped_grid_dir = os.path.join(save_dir, 'warped_grid')
      if not os.path.exists(warped_grid_dir):
          os.makedirs(warped_grid_dir)
      for step, inputs in enumerate(test_loader.data_loader):
          iter_start_time = time.time()

          c_names = inputs['c_name']
          im_names = inputs['im_name']
          im = inputs['image'].cuda()
          im_pose = inputs['pose_image'].cuda()
          im_h = inputs['head'].cuda()
          shape = inputs['shape'].cuda()
          agnostic = inputs['agnostic'].cuda()
          c = inputs['cloth'].cuda()
          cm = inputs['cloth_mask'].cuda()
          im_c = inputs['parse_cloth'].cuda()
          im_g = inputs['grid_image'].cuda()
          shape_ori = inputs['shape_ori']  # original body shape without blurring

          grid, theta = model(agnostic, cm)
          warped_cloth = F.grid_sample(c, grid, padding_mode='border')
          warped_mask = F.grid_sample(cm, grid, padding_mode='zeros')
          warped_grid = F.grid_sample(im_g, grid, padding_mode='zeros')
          overlay = 0.7 * warped_cloth + 0.3 * im

          visuals = [[im_h, shape, im_pose],
                    [c, warped_cloth, im_c],
                    [warped_grid, (warped_cloth+im)*0.5, im]]

          # save_images(warped_cloth, c_names, warp_cloth_dir)
          # save_images(warped_mask*2-1, c_names, warp_mask_dir)
          save_images(warped_cloth, im_names, warp_cloth_dir)
          save_images(warped_mask * 2 - 1, im_names, warp_mask_dir)
          save_images(shape_ori.cuda() * 0.2 + warped_cloth *
                      0.8, im_names, result_dir1)
          save_images(warped_grid, im_names, warped_grid_dir)
          save_images(overlay, im_names, overlayed_TPS_dir)

          if (step+1) % opt.display_count == 0:
              board_add_images(board, 'combine', visuals, step+1)
              t = time.time() - iter_start_time
              print('step: %8d, time: %.3f' % (step+1, t), flush=True)


  def test_tom(opt, test_loader, model, board):
      model.cuda()
      model.eval()

      base_name = os.path.basename(opt.checkpoint)
      # save_dir = os.path.join(opt.result_dir, base_name, opt.datamode)
      #save_dir = os.path.join(opt.result_dir, opt.name, opt.datamode)
      save_dir = opt.result_dir
      if not os.path.exists(save_dir):
          os.makedirs(save_dir)
      try_on_dir = os.path.join(save_dir, 'try2-on')
      if not os.path.exists(try_on_dir):
          os.makedirs(try_on_dir)
      p_rendered_dir = os.path.join(save_dir, 'p_rendered')
      if not os.path.exists(p_rendered_dir):
          os.makedirs(p_rendered_dir)
      m_composite_dir = os.path.join(save_dir, 'm_composite')
      if not os.path.exists(m_composite_dir):
          os.makedirs(m_composite_dir)
      im_pose_dir = os.path.join(save_dir, 'im_pose')
      if not os.path.exists(im_pose_dir):
          os.makedirs(im_pose_dir)
      shape_dir = os.path.join(save_dir, 'shape')
      if not os.path.exists(shape_dir):
          os.makedirs(shape_dir)
      im_h_dir = os.path.join(save_dir, 'im_h')
      if not os.path.exists(im_h_dir):
          os.makedirs(im_h_dir)  # for test data

      print('Dataset size: %05d!' % (len(test_loader.dataset)), flush=True)
      for step, inputs in enumerate(test_loader.data_loader):
          iter_start_time = time.time()

          im_names = inputs['im_name']
          im = inputs['image'].cuda()
          im_pose = inputs['pose_image']
          im_h = inputs['head']
          shape = inputs['shape']

          agnostic = inputs['agnostic'].cuda()
          c = inputs['cloth'].cuda()
          cm = inputs['cloth_mask'].cuda()

          # outputs = model(torch.cat([agnostic, c], 1))  # CP-VTON
          outputs = model(torch.cat([agnostic, c, cm], 1))  # CP-VTON+
          p_rendered, m_composite = torch.split(outputs, 3, 1)
          p_rendered = F.tanh(p_rendered)
          m_composite = F.sigmoid(m_composite)
          p_tryon = c * m_composite + p_rendered * (1 - m_composite)

          visuals = [[im_h, shape, im_pose],
                    [c, 2*cm-1, m_composite],
                    [p_rendered, p_tryon, im]]

          save_images(p_tryon, im_names, try_on_dir)
          save_images(im_h, im_names, im_h_dir)
          save_images(shape, im_names, shape_dir)
          save_images(im_pose, im_names, im_pose_dir)
          save_images(m_composite, im_names, m_composite_dir)
          save_images(p_rendered, im_names, p_rendered_dir)  # For test data

          if (step+1) % opt.display_count == 0:
              board_add_images(board, 'combine', visuals, step+1)
              t = time.time() - iter_start_time
              print('step: %8d, time: %.3f' % (step+1, t), flush=True)


  def Final_Testing():
      gopt = get_gopt()
      topt = get_topt()
      print(gopt)
      print(topt)
      print("Start to test stage: %s, named: %s!" % (gopt.stage, gopt.name))
      print("Start to test stage: %s, named: %s!" % (topt.stage, topt.name))
      # create dataset
      gtest_dataset = CPDataset(gopt)
      ttest_dataset = CPDataset(topt)
      # create dataloader
      gtest_loader = CPDataLoader(gopt, gtest_dataset)
      ttest_loader = CPDataLoader(topt, ttest_dataset)
      # visualization
      if not os.path.exists(gopt.tensorboard_dir):
          os.makedirs(gopt.tensorboard_dir)
      board = SummaryWriter(logdir=os.path.join(gopt.tensorboard_dir, gopt.name))

      # create model & test
      model = GMM(gopt)
      load_checkpoint(model, gopt.checkpoint)
      with torch.no_grad():
          test_gmm(gopt, gtest_loader, model, board)

      # model = UnetGenerator(25, 4, 6, ngf=64, norm_layer=nn.InstanceNorm2d)  # CP-VTON
      model = UnetGenerator(26, 4, 6, ngf=64, norm_layer=nn.InstanceNorm2d)  # CP-VTON+
      load_checkpoint(model, topt.checkpoint)
      with torch.no_grad():
          test_tom(topt, ttest_loader, model, board)
    

      print('Finished test %s, named: %s!' % (gopt.stage, gopt.name))
      print('Finished test %s, named: %s!' % (topt.stage, topt.name))






  Cloth_Mask("/content/drive/MyDrive/finalSmartfit/test/cloth/000010_1.jpg","/content/drive/MyDrive/finalSmartfit/test/cloth-mask/000010_1.jpg")
  Model_Mask("/content/drive/MyDrive/finalSmartfit/test/image/000001_0.jpg","/content/drive/MyDrive/finalSmartfit/test/image-mask/000001_0.png")
  generate_keypoints_json("/content/drive/MyDrive/finalSmartfit/test/image/000001_0.jpg","/content/drive/MyDrive/Testing_2_images/pose_deploy_linevec.prototxt","/content/drive/MyDrive/Uploads/pose_iter_440000.caffemodel",'/content/drive/MyDrive/finalSmartfit/test/pose/000001_0_keypoints.json')
  human_parsing('/content/drive/MyDrive/exp-schp-201908261155-lip.pth','/content/drive/MyDrive/finalSmartfit/test/image/','/content/drive/MyDrive/finalSmartfit/test/image-parse')
  Body_Segmentation()
  Body_Masking()
  Final_Testing()

app.run()

"""#SSIM"""

import tensorflow as tf
import cv2
# Read images from file.
im1 = cv2.imread('/content/drive/MyDrive/o.jpeg')
im2 = cv2.imread('/content/drive/MyDrive/u.jpeg')
# Compute SSIM over tf.uint8 Tensors.
ssim1 = tf.image.ssim(im1, im2, max_val=255, filter_size=11,filter_sigma=1.5, k1=0.01, k2=0.03)
print(ssim1)

!pip3 install scikit-image opencv-python imutils

!python3 /content/drive/MyDrive/ssim_matrics.py --first /content/drive/MyDrive/o.jpeg --second /content/drive/MyDrive/u.jpeg